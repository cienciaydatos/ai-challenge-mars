{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mars-anogan-keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBfhl2SDwf_f",
        "colab_type": "text"
      },
      "source": [
        "#Mount Google Drive\n",
        "\n",
        "The data used in this notebook is shared under the google drive link: https://drive.google.com/drive/folders/15AKvu7snthjZ46-OaXVv7SwK4wYyH7W0?usp=sharing\n",
        "\n",
        "Add this shared folder, OmdenaMars, to you google drive under path \"My Drive/aiml/\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gusMGRv8Dizz",
        "colab_type": "code",
        "outputId": "ecfaf5ba-0f03-4ce1-e2a8-8c91ebb77544",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "drive.mount('/content/gdrive', force_remount=False)\n",
        "GPATH=\"/content/gdrive/My Drive/aiml/OmdenaMars/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHPSzaJEFcAJ",
        "colab_type": "text"
      },
      "source": [
        "# Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtjeLS53DD-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE_PATH=\"/content/data/\"\n",
        "TRAIN_NATURAL_PATH=BASE_PATH+\"train/\"+\"natural/\"\n",
        "TEST_NATURAL_PATH=BASE_PATH+\"test/\"+\"natural/\"\n",
        "TEST_TECHNO_PATH=BASE_PATH+\"test/\"+\"techno/\"\n",
        "TMP_PATH=BASE_PATH+\"tmp/\"\n",
        "WEIGHTS_PATH=GPATH+\"weights/anogan/\"\n",
        "RESULTS_PATH=BASE_PATH+\"results/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkqNsVcjFE0L",
        "colab_type": "code",
        "outputId": "867dd280-528b-470c-9c79-efe90c6555aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import pathlib\n",
        "from shutil import copyfile\n",
        "import os\n",
        "\n",
        "pathlib.Path(TRAIN_NATURAL_PATH).mkdir(parents=True, exist_ok=True)\n",
        "pathlib.Path(TEST_NATURAL_PATH).mkdir(parents=True, exist_ok=True)\n",
        "pathlib.Path(TEST_TECHNO_PATH).mkdir(parents=True, exist_ok=True)\n",
        "pathlib.Path(TMP_PATH).mkdir(parents=True, exist_ok=True)\n",
        "pathlib.Path(WEIGHTS_PATH).mkdir(parents=True, exist_ok=True)\n",
        "pathlib.Path(RESULTS_PATH).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if not os.path.isfile(BASE_PATH+\"natural.tgz\"):\n",
        "    print(\"Copying natural.tgz to local folder\")\n",
        "    copyfile(GPATH+\"natural.tgz\", BASE_PATH+\"natural.tgz\")\n",
        "if not os.path.isfile(BASE_PATH+\"techno.tgz\"):\n",
        "    print(\"Copying techno.tgz to local folder\")\n",
        "    copyfile(GPATH+\"techno.tgz\", BASE_PATH+\"techno.tgz\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying natural.tgz to local folder\n",
            "Copying techno.tgz to local folder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORP70B5PUx1p",
        "colab_type": "code",
        "outputId": "92801a1a-ee75-4322-ba29-1d10d2bda1be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Cleanup old directories\n",
        "!rm -rf /content/data/train/natural/\n",
        "!rm -rf /content/data/test/natural/\n",
        "!rm -rf /content/data/results\n",
        "!mkdir /content/data/train/natural/\n",
        "!mkdir /content/data/test/natural/\n",
        "!mkdir /content/data/results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/data/results’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F7bL1iwF8lc",
        "colab_type": "code",
        "outputId": "84c9fa74-ca9d-4e29-d561-0ec8199394ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "print(\"Extracting techno.tgz\")\n",
        "os.system(\"tar -xzf \"+BASE_PATH+\"techno.tgz \"+\"-C \"+TMP_PATH)\n",
        "print(\"Extracting techno.tgz, Done.\")\n",
        "\n",
        "for dir in os.listdir(TMP_PATH+\"content/gdrive/My Drive/aiml/OmdenaMars/data/techno/\"):\n",
        "    print(\"Moving Files from:\", dir+\":\")\n",
        "    for file in os.listdir(TMP_PATH+\"content/gdrive/My Drive/aiml/OmdenaMars/data/techno/\"+dir):\n",
        "        #print(file)\n",
        "        shutil.move(TMP_PATH+\"content/gdrive/My Drive/aiml/OmdenaMars/data/techno/\"+dir+\"/\"+file, TEST_TECHNO_PATH+file)\n",
        "\n",
        "#cleanup temp dir\n",
        "shutil.rmtree(TMP_PATH+\"content\")\n",
        "\n",
        "print(\"Extracting natural.tgz\")\n",
        "os.system(\"tar -xzf \"+BASE_PATH+\"natural.tgz \"+\"-C \"+TMP_PATH)\n",
        "print(\"Extracting natural.tgz, Done.\")\n",
        "\n",
        "count = 204800\n",
        "print(\"Moving Natural File\")\n",
        "for file in os.listdir(TMP_PATH+\"content/gdrive/My Drive/aiml/OmdenaMars/data/natural/\"):\n",
        "    f1 = file.split(\"_\")\n",
        "    xVal = int(f1[2])\n",
        "    yVal = int(f1[4].split(\".\")[0])\n",
        "    if (xVal >= 8000) and (yVal>=5000):\n",
        "        if(xVal <= 15000) and (yVal <= 30000):\n",
        "            shutil.move(TMP_PATH+\"content/gdrive/My Drive/aiml/OmdenaMars/data/natural/\"+file, TRAIN_NATURAL_PATH+file)\n",
        "            count -= 1\n",
        "            if (count <= 0):\n",
        "                break\n",
        "\n",
        "print(\"Move 64 images from train/natural folder to test/natural folder\")\n",
        "count = 64\n",
        "for file in os.listdir(TRAIN_NATURAL_PATH):\n",
        "    shutil.move(TRAIN_NATURAL_PATH+file, TEST_NATURAL_PATH+file)\n",
        "    count -= 1\n",
        "    if count <= 0:\n",
        "        break\n",
        "print(\"Done\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting techno.tgz\n",
            "Extracting techno.tgz, Done.\n",
            "Moving Files from: 2-new-spots-spot2:\n",
            "Moving Files from: Curiosoity-Rover:\n",
            "Moving Files from: Heat-Sheild:\n",
            "Moving Files from: Descent-Stage-Crash-Site:\n",
            "Moving Files from: 2-new-spots-spot1:\n",
            "Moving Files from: Parachute:\n",
            "Extracting natural.tgz\n",
            "Extracting natural.tgz, Done.\n",
            "Moving Natural File\n",
            "Move 64 images from train/natural folder to test/natural folder\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c-7qWaJDU2m",
        "colab_type": "text"
      },
      "source": [
        "# AnoGAN\n",
        "\n",
        "From: https://github.com/yjucho1/anoGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtTjcWKDDWIk",
        "colab_type": "code",
        "outputId": "5ef45381-967d-41a4-cc55-cc2a00f343d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Reshape, Dense, Dropout, MaxPooling2D, Conv2D, Flatten\n",
        "from keras.layers import Conv2DTranspose, LeakyReLU\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras import backend as K\n",
        "from keras import initializers\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import math\n",
        "import csv\n",
        "\n",
        "from keras.utils. generic_utils import Progbar\n",
        " \n",
        "### combine images for visualization\n",
        "def combine_images(generated_images):\n",
        "    num = generated_images.shape[0]\n",
        "    width = int(math.sqrt(num))\n",
        "    height = int(math.ceil(float(num)/width))\n",
        "    shape = generated_images.shape[1:4]\n",
        "    image = np.zeros((height*shape[0], width*shape[1], shape[2]),\n",
        "                     dtype=generated_images.dtype)\n",
        "    for index, img in enumerate(generated_images):\n",
        "        i = int(index/width)\n",
        "        j = index % width\n",
        "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1],:] = img[:, :, :]\n",
        "    return image\n",
        " \n",
        "### generator model define\n",
        "def generator_model():\n",
        "    inputs = Input((100,))\n",
        "    #M#fc1 = Dense(input_dim=10, units=128*7*7)(inputs)\n",
        "    fc1 = Dense(input_dim=10, units=128*32*32)(inputs)\n",
        "    fc1 = BatchNormalization()(fc1)\n",
        "    fc1 = LeakyReLU(0.2)(fc1)\n",
        "    #M#fc2 = Reshape((7, 7, 128), input_shape=(128*7*7,))(fc1)\n",
        "    fc2 = Reshape((32, 32, 128), input_shape=(128*64*64,))(fc1)\n",
        "    up1 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(fc2)\n",
        "    conv1 = Conv2D(64, (3, 3), padding='same')(up1)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Activation('relu')(conv1)\n",
        "    up2 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv1)\n",
        "    conv2 = Conv2D(1, (5, 5), padding='same')(up2)\n",
        "    outputs = Activation('tanh')(conv2)\n",
        "     \n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "    return model\n",
        " \n",
        "### discriminator model define\n",
        "def discriminator_model():\n",
        "    #M#inputs = Input((28, 28, 1))\n",
        "    inputs = Input((128, 128, 1))\n",
        "    conv1 = Conv2D(64, (5, 5), padding='same')(inputs)\n",
        "    conv1 = LeakyReLU(0.2)(conv1)\n",
        "    conv1 = Dropout(0.2)(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(128, (5, 5), padding='same')(pool1)\n",
        "    conv2 = LeakyReLU(0.2)(conv2)\n",
        "    conv2 = Dropout(0.2)(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    \n",
        "    fc1 = Flatten()(pool2)\n",
        "    fc1 = Dense(1)(fc1)\n",
        "    outputs = Activation('sigmoid')(fc1)\n",
        "     \n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "    return model\n",
        " \n",
        "### d_on_g model for training generator\n",
        "def generator_containing_discriminator(g, d):\n",
        "    d.trainable = False\n",
        "    ganInput = Input(shape=(100,))\n",
        "    x = g(ganInput)\n",
        "    ganOutput = d(x)\n",
        "    gan = Model(inputs=ganInput, outputs=ganOutput)\n",
        "    #gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return gan\n",
        " \n",
        "def load_generator_model(best=False):\n",
        "    g = generator_model()\n",
        "    g_optim = RMSprop(lr=0.0002)\n",
        "    g.compile(loss='binary_crossentropy', optimizer=g_optim,metrics=['accuracy'])\n",
        "    if best == True:\n",
        "        g.load_weights(WEIGHTS_PATH+'generator.best.h5')\n",
        "    else:\n",
        "        g.load_weights(WEIGHTS_PATH+'generator.h5')\n",
        "    return g\n",
        "\n",
        "def load_discriminator_model(best=False):\n",
        "    d = discriminator_model()\n",
        "    d_optim = RMSprop(lr=0.0004)\n",
        "    d.compile(loss='binary_crossentropy', optimizer=d_optim, metrics=['accuracy'])\n",
        "    if best == True:\n",
        "        d.load_weights(WEIGHTS_PATH+'discriminator.best.h5')\n",
        "    else:\n",
        "        d.load_weights(WEIGHTS_PATH+'discriminator.h5')\n",
        "    return d\n",
        "\n",
        "def load_gan_model(g, d):\n",
        "    d_on_g = generator_containing_discriminator(g, d)\n",
        "    d_on_g.compile(loss='binary_crossentropy', optimizer=g_optim,metrics=['accuracy'])\n",
        "    d_on_g.load_weights(WEIGHTS_PATH+'gan.h5')\n",
        "    return g\n",
        "\n",
        "\n",
        "def load_model(best=False):\n",
        "    g = load_generator_model(best)\n",
        "    d = discriminator_model(best)\n",
        "    return g,d\n",
        "\n",
        "### train generator and discriminator\n",
        "def train(BATCH_SIZE, X_train_generator, reloadWeights=False):\n",
        "    \n",
        "    batch_size = X_train_generator.batch_size        \n",
        "    n_steps = int(X_train_generator.samples / batch_size)\n",
        "\n",
        "    ### model define\n",
        "    if reloadWeights == False:\n",
        "        d = discriminator_model()\n",
        "    else:\n",
        "        d= load_discriminator_model()\n",
        "        \n",
        "    if reloadWeights == False:\n",
        "        g = generator_model()\n",
        "    else:\n",
        "        g = load_generator_model()\n",
        "    \n",
        "    if reloadWeights == False:\n",
        "        d_on_g = generator_containing_discriminator(g, d)\n",
        "    else:\n",
        "        d_on_g = load_gan_model()\n",
        "        \n",
        "    d_optim = RMSprop(lr=0.0004)\n",
        "    g_optim = RMSprop(lr=0.0002)\n",
        "    \n",
        "    if reloadWeights == False:\n",
        "        #M#g.compile(loss='mse', optimizer=g_optim,metrics=['accuracy'])\n",
        "        g.compile(loss='binary_crossentropy', optimizer=g_optim,metrics=['accuracy'])\n",
        "    if reloadWeights == False:\n",
        "        #M#d_on_g.compile(loss='mse', optimizer=g_optim, metrics=['accuracy'])\n",
        "        d_on_g.compile(loss='binary_crossentropy', optimizer=g_optim,metrics=['accuracy'])\n",
        "    d.trainable = True\n",
        "    if reloadWeights == False:\n",
        "        #M#d.compile(loss='mse', optimizer=d_optim, metrics=['accuracy'])\n",
        "        d.compile(loss='binary_crossentropy', optimizer=d_optim, metrics=['accuracy'])\n",
        "     \n",
        "    if (not os.path.isfile(WEIGHTS_PATH+\"train-status.csv\")) or (reloadWeights == False):\n",
        "        #Initialize train status file\n",
        "        field_names = ['epoch', 'g-loss', 'g-acc', 'd-loss', 'd-acc']\n",
        "        with open(WEIGHTS_PATH+\"train-status.csv\",\"w\") as trainStatusFile:\n",
        "            tStatusLogger = csv.DictWriter(trainStatusFile, fieldnames=field_names)\n",
        "            tStatusLogger.writeheader()\n",
        "\n",
        "    bestDAcc = 0\n",
        "    bestGLoss = 100\n",
        "    for epoch in range(300):\n",
        "        print (\"Epoch is\", epoch)\n",
        "        #n_iter = int(X_train.shape[0]/BATCH_SIZE)\n",
        "        n_iter = int(X_train_generator.samples / BATCH_SIZE)\n",
        "        progress_bar = Progbar(target=n_iter)\n",
        "        histGLoss = []\n",
        "        histGAcc = []\n",
        "        histDLoss = []\n",
        "        histDAcc = []\n",
        "        for index in range(n_iter):\n",
        "            # create random noise -> U(0,1) 10 latent vectors\n",
        "            noise = np.random.uniform(0, 1, size=(BATCH_SIZE, 100))\n",
        " \n",
        "            # load real data & generate fake data\n",
        "            #image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
        "            image_batch = (X_train_generator.next().astype(np.float32) - 127.5) / 127.5\n",
        "            if (image_batch.shape[0] != BATCH_SIZE):\n",
        "                #Incomplete last batch, skip to next batch\n",
        "                image_batch = (X_train_generator.next().astype(np.float32) - 127.5) / 127.5\n",
        "                \n",
        "            generated_images = g.predict(noise, verbose=0)\n",
        "             \n",
        "            # visualize training results\n",
        "            if (epoch % 5 == 0) and (index== n_iter-1):\n",
        "                image = combine_images(generated_images)\n",
        "                image = image*127.5+127.5\n",
        "                cv2.imwrite(RESULTS_PATH+str(epoch)+\"_\"+str(index)+\".png\", image)\n",
        " \n",
        "            # attach label for training discriminator\n",
        "            X = np.concatenate((image_batch, generated_images))\n",
        "            y = np.array([1] * BATCH_SIZE + [0] * BATCH_SIZE)\n",
        "             \n",
        "            # training discriminator\n",
        "            d_loss = d.train_on_batch(X, y)\n",
        " \n",
        "            # training generator\n",
        "            d.trainable = False\n",
        "            g_loss = d_on_g.train_on_batch(noise, np.array([1] * BATCH_SIZE))\n",
        "            d.trainable = True\n",
        " \n",
        "            progress_bar.update(index+1, values=[('g-loss:',g_loss[0]), ('g-acc',g_loss[1]), ('d-loss:',d_loss[0]),('d-acc:',d_loss[1])])\n",
        "            histGLoss.append(g_loss[0])\n",
        "            histGAcc.append(g_loss[1])\n",
        "            histDLoss.append(d_loss[0])\n",
        "            histDAcc.append(d_loss[1])\n",
        " \n",
        "        #save train status\n",
        "        tStatus = {}\n",
        "        tStatus['epoch'] = epoch\n",
        "        tStatus['g-loss'] = sum(histGLoss)/len(histGLoss)\n",
        "        tStatus['g-acc'] = sum(histGAcc)/len(histGAcc)\n",
        "        tStatus['d-loss'] = sum(histDLoss)/len(histDLoss)\n",
        "        tStatus['d-acc'] = sum(histDAcc)/len(histDAcc)\n",
        "\n",
        "        with open(WEIGHTS_PATH+\"train-status.csv\",\"a\") as trainStatusFile:\n",
        "            tStatusLogger = csv.DictWriter(trainStatusFile, fieldnames=field_names)\n",
        "            tStatusLogger.writerow(tStatus)\n",
        "\n",
        "        # save weights for each epoch\n",
        "        g.save_weights(WEIGHTS_PATH+'generator.h5', True)\n",
        "        d.save_weights(WEIGHTS_PATH+'discriminator.h5', True)\n",
        "        d_on_g.save_weights(WEIGHTS_PATH+'gan.h5', True)\n",
        "        \n",
        "        d1 = tStatus['d-acc'] + (1-tStatus['g-loss'])\n",
        "        if d1 > bestDAcc:\n",
        "            bestDAcc = d1\n",
        "            with open(WEIGHTS_PATH+\"discriminator.best.txt\", \"w\") as file:\n",
        "                file.write(\"Epoch: {} discriminator best acc: {}\".format(epoch, d1))\n",
        "            d.save_weights(WEIGHTS_PATH+'discriminator.best.h5', True)\n",
        "            \n",
        "        if ((sum(histGLoss)/len(histGLoss)) < bestGLoss):\n",
        "            bestGLoss = (sum(histGLoss)/len(histGLoss))\n",
        "            with open(WEIGHTS_PATH+\"generator.best.txt\", \"w\") as file:\n",
        "                file.write(\"Epoch: {} generator best loss: {}\".format(epoch, bestGLoss))\n",
        "            g.save_weights(WEIGHTS_PATH+'generator.best.h5', True)\n",
        "            \n",
        "    return d, g\n",
        " \n",
        "### generate images\n",
        "def generate(BATCH_SIZE):\n",
        "    g = load_generator_model()\n",
        "    noise = np.random.uniform(0, 1, (BATCH_SIZE, 100))\n",
        "    generated_images = g.predict(noise)\n",
        "    return generated_images\n",
        " \n",
        "### anomaly loss function \n",
        "def sum_of_residual(y_true, y_pred):\n",
        "    return K.sum(K.abs(y_true - y_pred))\n",
        " \n",
        "### discriminator intermediate layer feautre extraction\n",
        "def feature_extractor(d=None):\n",
        "    if d is None:\n",
        "        d = load_discriminator_model()\n",
        "    intermidiate_model = Model(inputs=d.layers[0].input, outputs=d.layers[-7].output)\n",
        "    intermidiate_model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
        "    return intermidiate_model\n",
        " \n",
        "### anomaly detection model define\n",
        "def anomaly_detector(g=None, d=None):\n",
        "    if g is None:\n",
        "        g = load_generator_model()\n",
        "    if d is None:\n",
        "        d = load_discriminator_model()\n",
        "        \n",
        "    intermidiate_model = feature_extractor(d)\n",
        "    intermidiate_model.trainable = False\n",
        "    g = Model(inputs=g.layers[1].input, outputs=g.layers[-1].output)\n",
        "    g.trainable = False\n",
        "    # Input layer cann't be trained. Add new layer as same size & same distribution\n",
        "    aInput = Input(shape=(100,))\n",
        "    gInput = Dense((100), trainable=True)(aInput)\n",
        "    gInput = Activation('sigmoid')(gInput)\n",
        "     \n",
        "    # G & D feature\n",
        "    G_out = g(gInput)\n",
        "    D_out= intermidiate_model(G_out)    \n",
        "    model = Model(inputs=aInput, outputs=[G_out, D_out])\n",
        "    model.compile(loss=sum_of_residual, loss_weights= [0.90, 0.10], optimizer='rmsprop')\n",
        "     \n",
        "    # batchnorm learning phase fixed (test) : make non trainable\n",
        "    K.set_learning_phase(0)\n",
        "     \n",
        "    return model\n",
        " \n",
        "### anomaly detection\n",
        "def compute_anomaly_score(model, x, iterations=500, d=None):\n",
        "    z = np.random.uniform(0, 1, size=(1, 100))\n",
        "     \n",
        "    intermidiate_model = feature_extractor(d)\n",
        "    d_x = intermidiate_model.predict(x)\n",
        " \n",
        "    # learning for changing latent\n",
        "    loss = model.fit(z, [x, d_x], batch_size=1, epochs=iterations, verbose=0)\n",
        "    similar_data, _ = model.predict(z)\n",
        "     \n",
        "    loss = loss.history['loss'][-1]\n",
        "     \n",
        "    return loss, similar_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDzARAccR9q-",
        "colab_type": "code",
        "outputId": "d4056bdf-f0bf-4862-d6ef-e4f35117f1c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1565
        }
      },
      "source": [
        "print(\"generator_model:\")\n",
        "t1 = generator_model()\n",
        "print(t1.summary())\n",
        "\n",
        "print(\"discriminator_model:\")\n",
        "t1 = discriminator_model()\n",
        "print(t1.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0617 16:21:58.375554 140504535340928 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0617 16:21:58.564907 140504535340928 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0617 16:21:58.578227 140504535340928 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "generator_model:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0617 16:21:58.672552 140504535340928 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0617 16:21:58.753432 140504535340928 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0617 16:22:01.788497 140504535340928 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0617 16:22:01.906293 140504535340928 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0617 16:22:01.921438 140504535340928 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 131072)            13238272  \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 131072)            524288    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 131072)            0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 64, 64, 64)        32832     \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 64, 64, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 64, 64, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 128, 128, 64)      16448     \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 128, 128, 1)       1601      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 128, 128, 1)       0         \n",
            "=================================================================\n",
            "Total params: 13,850,625\n",
            "Trainable params: 13,588,353\n",
            "Non-trainable params: 262,272\n",
            "_________________________________________________________________\n",
            "None\n",
            "discriminator_model:\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 128, 128, 1)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 128, 128, 64)      1664      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 64, 64, 128)       204928    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 131072)            0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 131073    \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 337,665\n",
            "Trainable params: 337,665\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCb-DTp_Gn6e",
        "colab_type": "text"
      },
      "source": [
        "## Train Ano-GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP25VAPhDevq",
        "colab_type": "code",
        "outputId": "b636592b-fd98-4347-ac25-436b12514155",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "train_directory=BASE_PATH+\"train/\"\n",
        "validation_directory=BASE_PATH+\"test/\"\n",
        "\n",
        "train_datagen = ImageDataGenerator()\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory   = train_directory,       # this is the target directory\n",
        "    target_size = (128, 128, 1)[:-1],         # all images will be resized to 64x64\n",
        "    batch_size  = 128,\n",
        "    color_mode  = \"grayscale\",                    # We use a grayscale dataset\n",
        "    classes=[\"natural\"],\n",
        "    class_mode  = None                            # We do not need to get any label => Everything is healthy\n",
        ")  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10657 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0OmuUq9GyN5",
        "colab_type": "code",
        "outputId": "e9f918b8-4a9f-422b-d4a7-3b8d889dc028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11037
        }
      },
      "source": [
        "#%debug\n",
        "Model_d, Model_g = train(128, train_generator)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0617 16:22:18.472947 140504535340928 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0617 16:22:18.485135 140504535340928 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch is 0\n",
            "83/83 [==============================] - 110s 1s/step - g-loss:: 2.0730 - g-acc: 0.3890 - d-loss:: 0.6977 - d-acc:: 0.7136\n",
            "Epoch is 1\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1819 - g-acc: 0.9596 - d-loss:: 0.5389 - d-acc:: 0.7260\n",
            "Epoch is 2\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2480 - g-acc: 0.9661 - d-loss:: 0.6235 - d-acc:: 0.6579\n",
            "Epoch is 3\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.0720 - g-acc: 0.9986 - d-loss:: 0.3952 - d-acc:: 0.8261\n",
            "Epoch is 4\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.0160 - g-acc: 1.0000 - d-loss:: 0.3499 - d-acc:: 0.8350\n",
            "Epoch is 5\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.0219 - g-acc: 0.9945 - d-loss:: 0.3062 - d-acc:: 0.8783\n",
            "Epoch is 6\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.0690 - g-acc: 0.9884 - d-loss:: 0.5175 - d-acc:: 0.7335\n",
            "Epoch is 7\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.0709 - g-acc: 0.9908 - d-loss:: 0.5082 - d-acc:: 0.7613\n",
            "Epoch is 8\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.0383 - g-acc: 0.9879 - d-loss:: 0.3131 - d-acc:: 0.8777\n",
            "Epoch is 9\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.0185 - g-acc: 1.0000 - d-loss:: 0.3190 - d-acc:: 0.8684\n",
            "Epoch is 10\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.0558 - g-acc: 0.9936 - d-loss:: 0.5048 - d-acc:: 0.7596\n",
            "Epoch is 11\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.0719 - g-acc: 0.9896 - d-loss:: 0.5581 - d-acc:: 0.7089\n",
            "Epoch is 12\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.0612 - g-acc: 0.9996 - d-loss:: 0.5257 - d-acc:: 0.7210\n",
            "Epoch is 13\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.0114 - g-acc: 0.9990 - d-loss:: 0.2010 - d-acc:: 0.9202\n",
            "Epoch is 14\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.0882 - g-acc: 0.9816 - d-loss:: 0.4967 - d-acc:: 0.7580\n",
            "Epoch is 15\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1337 - g-acc: 0.9643 - d-loss:: 0.5631 - d-acc:: 0.7107\n",
            "Epoch is 16\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.0858 - g-acc: 0.9888 - d-loss:: 0.4998 - d-acc:: 0.7558\n",
            "Epoch is 17\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.0845 - g-acc: 0.9800 - d-loss:: 0.5271 - d-acc:: 0.7447\n",
            "Epoch is 18\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.0949 - g-acc: 0.9814 - d-loss:: 0.5575 - d-acc:: 0.7208\n",
            "Epoch is 19\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2868 - g-acc: 0.9323 - d-loss:: 0.6856 - d-acc:: 0.5946\n",
            "Epoch is 20\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2329 - g-acc: 0.9720 - d-loss:: 0.6300 - d-acc:: 0.6245\n",
            "Epoch is 21\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.0945 - g-acc: 0.9912 - d-loss:: 0.5503 - d-acc:: 0.7308\n",
            "Epoch is 22\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.1191 - g-acc: 0.9631 - d-loss:: 0.5395 - d-acc:: 0.7421\n",
            "Epoch is 23\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2408 - g-acc: 0.9647 - d-loss:: 0.6809 - d-acc:: 0.5920\n",
            "Epoch is 24\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2872 - g-acc: 0.9614 - d-loss:: 0.6808 - d-acc:: 0.5628\n",
            "Epoch is 25\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2388 - g-acc: 0.9637 - d-loss:: 0.6520 - d-acc:: 0.6019\n",
            "Epoch is 26\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.3020 - g-acc: 0.9458 - d-loss:: 0.6875 - d-acc:: 0.5713\n",
            "Epoch is 27\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2136 - g-acc: 0.9614 - d-loss:: 0.6243 - d-acc:: 0.6321\n",
            "Epoch is 28\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2190 - g-acc: 0.9691 - d-loss:: 0.6412 - d-acc:: 0.6191\n",
            "Epoch is 29\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2168 - g-acc: 0.9752 - d-loss:: 0.6068 - d-acc:: 0.6418\n",
            "Epoch is 30\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1779 - g-acc: 0.9589 - d-loss:: 0.5488 - d-acc:: 0.7087\n",
            "Epoch is 31\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1822 - g-acc: 0.9663 - d-loss:: 0.6189 - d-acc:: 0.6641\n",
            "Epoch is 32\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1977 - g-acc: 0.9707 - d-loss:: 0.6127 - d-acc:: 0.6319\n",
            "Epoch is 33\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1674 - g-acc: 0.9862 - d-loss:: 0.5955 - d-acc:: 0.6554\n",
            "Epoch is 34\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1610 - g-acc: 0.9942 - d-loss:: 0.6301 - d-acc:: 0.6167\n",
            "Epoch is 35\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2296 - g-acc: 0.9691 - d-loss:: 0.6411 - d-acc:: 0.6087\n",
            "Epoch is 36\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2381 - g-acc: 0.9592 - d-loss:: 0.6608 - d-acc:: 0.5976\n",
            "Epoch is 37\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2490 - g-acc: 0.9841 - d-loss:: 0.6465 - d-acc:: 0.5851\n",
            "Epoch is 38\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2492 - g-acc: 0.9761 - d-loss:: 0.6583 - d-acc:: 0.5820\n",
            "Epoch is 39\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1836 - g-acc: 0.9833 - d-loss:: 0.6073 - d-acc:: 0.6351\n",
            "Epoch is 40\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2620 - g-acc: 0.9541 - d-loss:: 0.6538 - d-acc:: 0.5984\n",
            "Epoch is 41\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2325 - g-acc: 0.9784 - d-loss:: 0.6514 - d-acc:: 0.6022\n",
            "Epoch is 42\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2172 - g-acc: 0.9833 - d-loss:: 0.6443 - d-acc:: 0.6050\n",
            "Epoch is 43\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2336 - g-acc: 0.9637 - d-loss:: 0.6379 - d-acc:: 0.6194\n",
            "Epoch is 44\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1185 - g-acc: 0.9865 - d-loss:: 0.4935 - d-acc:: 0.7449\n",
            "Epoch is 45\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1781 - g-acc: 0.9477 - d-loss:: 0.5649 - d-acc:: 0.7045\n",
            "Epoch is 46\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1829 - g-acc: 0.9639 - d-loss:: 0.6472 - d-acc:: 0.6322\n",
            "Epoch is 47\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2224 - g-acc: 0.9686 - d-loss:: 0.6523 - d-acc:: 0.6014\n",
            "Epoch is 48\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2700 - g-acc: 0.9533 - d-loss:: 0.6573 - d-acc:: 0.5921\n",
            "Epoch is 49\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2398 - g-acc: 0.9763 - d-loss:: 0.6376 - d-acc:: 0.6050\n",
            "Epoch is 50\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2593 - g-acc: 0.9674 - d-loss:: 0.6385 - d-acc:: 0.6042\n",
            "Epoch is 51\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2589 - g-acc: 0.9584 - d-loss:: 0.6434 - d-acc:: 0.6066\n",
            "Epoch is 52\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1911 - g-acc: 0.9790 - d-loss:: 0.5843 - d-acc:: 0.6572\n",
            "Epoch is 53\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2185 - g-acc: 0.9753 - d-loss:: 0.6424 - d-acc:: 0.6102\n",
            "Epoch is 54\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2847 - g-acc: 0.9387 - d-loss:: 0.6741 - d-acc:: 0.5762\n",
            "Epoch is 55\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2209 - g-acc: 0.9699 - d-loss:: 0.6364 - d-acc:: 0.6219\n",
            "Epoch is 56\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2506 - g-acc: 0.9557 - d-loss:: 0.6300 - d-acc:: 0.6121\n",
            "Epoch is 57\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1989 - g-acc: 0.9847 - d-loss:: 0.6002 - d-acc:: 0.6467\n",
            "Epoch is 58\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.1500 - g-acc: 0.9849 - d-loss:: 0.5846 - d-acc:: 0.6736\n",
            "Epoch is 59\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2405 - g-acc: 0.9583 - d-loss:: 0.6467 - d-acc:: 0.6067\n",
            "Epoch is 60\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2076 - g-acc: 0.9872 - d-loss:: 0.6309 - d-acc:: 0.6137\n",
            "Epoch is 61\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2316 - g-acc: 0.9723 - d-loss:: 0.6578 - d-acc:: 0.6008\n",
            "Epoch is 62\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.1770 - g-acc: 0.9900 - d-loss:: 0.5849 - d-acc:: 0.6626\n",
            "Epoch is 63\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2501 - g-acc: 0.9628 - d-loss:: 0.6817 - d-acc:: 0.5821\n",
            "Epoch is 64\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2431 - g-acc: 0.9677 - d-loss:: 0.6329 - d-acc:: 0.6170\n",
            "Epoch is 65\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2573 - g-acc: 0.9354 - d-loss:: 0.6212 - d-acc:: 0.6276\n",
            "Epoch is 66\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1127 - g-acc: 0.9878 - d-loss:: 0.5017 - d-acc:: 0.7415\n",
            "Epoch is 67\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2595 - g-acc: 0.9527 - d-loss:: 0.6911 - d-acc:: 0.5707\n",
            "Epoch is 68\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2005 - g-acc: 0.9695 - d-loss:: 0.5777 - d-acc:: 0.6635\n",
            "Epoch is 69\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1650 - g-acc: 0.9917 - d-loss:: 0.6209 - d-acc:: 0.6349\n",
            "Epoch is 70\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1478 - g-acc: 0.9972 - d-loss:: 0.5744 - d-acc:: 0.6670\n",
            "Epoch is 71\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2339 - g-acc: 0.9586 - d-loss:: 0.6618 - d-acc:: 0.6080\n",
            "Epoch is 72\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1183 - g-acc: 0.9980 - d-loss:: 0.5107 - d-acc:: 0.7209\n",
            "Epoch is 73\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1800 - g-acc: 0.9887 - d-loss:: 0.6316 - d-acc:: 0.6313\n",
            "Epoch is 74\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2362 - g-acc: 0.9789 - d-loss:: 0.6195 - d-acc:: 0.6248\n",
            "Epoch is 75\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2147 - g-acc: 0.9884 - d-loss:: 0.6384 - d-acc:: 0.6089\n",
            "Epoch is 76\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2629 - g-acc: 0.9631 - d-loss:: 0.6748 - d-acc:: 0.5816\n",
            "Epoch is 77\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2448 - g-acc: 0.9670 - d-loss:: 0.6367 - d-acc:: 0.6137\n",
            "Epoch is 78\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2645 - g-acc: 0.9563 - d-loss:: 0.6507 - d-acc:: 0.6113\n",
            "Epoch is 79\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2327 - g-acc: 0.9797 - d-loss:: 0.6376 - d-acc:: 0.6125\n",
            "Epoch is 80\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2696 - g-acc: 0.9536 - d-loss:: 0.6532 - d-acc:: 0.5980\n",
            "Epoch is 81\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2191 - g-acc: 0.9818 - d-loss:: 0.6128 - d-acc:: 0.6434\n",
            "Epoch is 82\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2542 - g-acc: 0.9671 - d-loss:: 0.6342 - d-acc:: 0.6129\n",
            "Epoch is 83\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2238 - g-acc: 0.9724 - d-loss:: 0.6363 - d-acc:: 0.6211\n",
            "Epoch is 84\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2667 - g-acc: 0.9522 - d-loss:: 0.6254 - d-acc:: 0.6147\n",
            "Epoch is 85\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2236 - g-acc: 0.9704 - d-loss:: 0.6204 - d-acc:: 0.6374\n",
            "Epoch is 86\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2173 - g-acc: 0.9816 - d-loss:: 0.6020 - d-acc:: 0.6452\n",
            "Epoch is 87\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1895 - g-acc: 0.9832 - d-loss:: 0.5968 - d-acc:: 0.6572\n",
            "Epoch is 88\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2165 - g-acc: 0.9763 - d-loss:: 0.5753 - d-acc:: 0.6661\n",
            "Epoch is 89\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.1457 - g-acc: 0.9959 - d-loss:: 0.5667 - d-acc:: 0.6853\n",
            "Epoch is 90\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2404 - g-acc: 0.9593 - d-loss:: 0.6568 - d-acc:: 0.6099\n",
            "Epoch is 91\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1982 - g-acc: 0.9847 - d-loss:: 0.6068 - d-acc:: 0.6469\n",
            "Epoch is 92\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.1550 - g-acc: 0.9930 - d-loss:: 0.5428 - d-acc:: 0.7012\n",
            "Epoch is 93\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1688 - g-acc: 0.9887 - d-loss:: 0.6099 - d-acc:: 0.6468\n",
            "Epoch is 94\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2430 - g-acc: 0.9715 - d-loss:: 0.6048 - d-acc:: 0.6406\n",
            "Epoch is 95\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2147 - g-acc: 0.9764 - d-loss:: 0.5884 - d-acc:: 0.6607\n",
            "Epoch is 96\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1311 - g-acc: 0.9868 - d-loss:: 0.5098 - d-acc:: 0.7405\n",
            "Epoch is 97\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1630 - g-acc: 0.9914 - d-loss:: 0.5633 - d-acc:: 0.6937\n",
            "Epoch is 98\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.1977 - g-acc: 0.9761 - d-loss:: 0.5819 - d-acc:: 0.6710\n",
            "Epoch is 99\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2582 - g-acc: 0.9585 - d-loss:: 0.6285 - d-acc:: 0.6234\n",
            "Epoch is 100\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1905 - g-acc: 0.9818 - d-loss:: 0.5769 - d-acc:: 0.6784\n",
            "Epoch is 101\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2222 - g-acc: 0.9695 - d-loss:: 0.6238 - d-acc:: 0.6382\n",
            "Epoch is 102\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1888 - g-acc: 0.9858 - d-loss:: 0.5611 - d-acc:: 0.6869\n",
            "Epoch is 103\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2430 - g-acc: 0.9624 - d-loss:: 0.6312 - d-acc:: 0.6306\n",
            "Epoch is 104\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2057 - g-acc: 0.9769 - d-loss:: 0.5581 - d-acc:: 0.6947\n",
            "Epoch is 105\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2032 - g-acc: 0.9779 - d-loss:: 0.5535 - d-acc:: 0.6946\n",
            "Epoch is 106\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1752 - g-acc: 0.9827 - d-loss:: 0.5182 - d-acc:: 0.7196\n",
            "Epoch is 107\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.1729 - g-acc: 0.9848 - d-loss:: 0.5265 - d-acc:: 0.7174\n",
            "Epoch is 108\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1160 - g-acc: 0.9915 - d-loss:: 0.5007 - d-acc:: 0.7473\n",
            "Epoch is 109\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1653 - g-acc: 0.9786 - d-loss:: 0.5592 - d-acc:: 0.7011\n",
            "Epoch is 110\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2130 - g-acc: 0.9774 - d-loss:: 0.5973 - d-acc:: 0.6614\n",
            "Epoch is 111\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1848 - g-acc: 0.9697 - d-loss:: 0.5788 - d-acc:: 0.6822\n",
            "Epoch is 112\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2715 - g-acc: 0.9277 - d-loss:: 0.6100 - d-acc:: 0.6533\n",
            "Epoch is 113\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2215 - g-acc: 0.9697 - d-loss:: 0.5984 - d-acc:: 0.6557\n",
            "Epoch is 114\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2394 - g-acc: 0.9475 - d-loss:: 0.5853 - d-acc:: 0.6680\n",
            "Epoch is 115\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1894 - g-acc: 0.9743 - d-loss:: 0.5237 - d-acc:: 0.7203\n",
            "Epoch is 116\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2096 - g-acc: 0.9704 - d-loss:: 0.5594 - d-acc:: 0.6936\n",
            "Epoch is 117\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2651 - g-acc: 0.9329 - d-loss:: 0.6007 - d-acc:: 0.6643\n",
            "Epoch is 118\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2727 - g-acc: 0.9256 - d-loss:: 0.5978 - d-acc:: 0.6608\n",
            "Epoch is 119\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.1910 - g-acc: 0.9731 - d-loss:: 0.5409 - d-acc:: 0.7038\n",
            "Epoch is 120\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2044 - g-acc: 0.9599 - d-loss:: 0.5858 - d-acc:: 0.6717\n",
            "Epoch is 121\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2428 - g-acc: 0.9350 - d-loss:: 0.5325 - d-acc:: 0.7247\n",
            "Epoch is 122\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2815 - g-acc: 0.9236 - d-loss:: 0.5960 - d-acc:: 0.6701\n",
            "Epoch is 123\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2532 - g-acc: 0.9504 - d-loss:: 0.5804 - d-acc:: 0.6741\n",
            "Epoch is 124\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2355 - g-acc: 0.9496 - d-loss:: 0.5797 - d-acc:: 0.6751\n",
            "Epoch is 125\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2720 - g-acc: 0.9275 - d-loss:: 0.5983 - d-acc:: 0.6554\n",
            "Epoch is 126\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2722 - g-acc: 0.9365 - d-loss:: 0.5904 - d-acc:: 0.6619\n",
            "Epoch is 127\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2375 - g-acc: 0.9564 - d-loss:: 0.5629 - d-acc:: 0.6833\n",
            "Epoch is 128\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.1266 - g-acc: 0.9917 - d-loss:: 0.4634 - d-acc:: 0.7666\n",
            "Epoch is 129\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2149 - g-acc: 0.9607 - d-loss:: 0.5486 - d-acc:: 0.7054\n",
            "Epoch is 130\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.3222 - g-acc: 0.8741 - d-loss:: 0.6230 - d-acc:: 0.6392\n",
            "Epoch is 131\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2459 - g-acc: 0.9420 - d-loss:: 0.5709 - d-acc:: 0.6802\n",
            "Epoch is 132\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.1297 - g-acc: 0.9950 - d-loss:: 0.5115 - d-acc:: 0.7293\n",
            "Epoch is 133\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2113 - g-acc: 0.9736 - d-loss:: 0.5812 - d-acc:: 0.6761\n",
            "Epoch is 134\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2256 - g-acc: 0.9629 - d-loss:: 0.5620 - d-acc:: 0.6929\n",
            "Epoch is 135\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2430 - g-acc: 0.9525 - d-loss:: 0.5816 - d-acc:: 0.6775\n",
            "Epoch is 136\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2660 - g-acc: 0.9394 - d-loss:: 0.6071 - d-acc:: 0.6530\n",
            "Epoch is 137\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.3035 - g-acc: 0.9153 - d-loss:: 0.6355 - d-acc:: 0.6240\n",
            "Epoch is 138\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.3027 - g-acc: 0.9187 - d-loss:: 0.6044 - d-acc:: 0.6517\n",
            "Epoch is 139\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.3298 - g-acc: 0.8884 - d-loss:: 0.5924 - d-acc:: 0.6596\n",
            "Epoch is 140\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2504 - g-acc: 0.9363 - d-loss:: 0.5650 - d-acc:: 0.6913\n",
            "Epoch is 141\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2837 - g-acc: 0.9267 - d-loss:: 0.5752 - d-acc:: 0.6814\n",
            "Epoch is 142\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2407 - g-acc: 0.9519 - d-loss:: 0.5703 - d-acc:: 0.6815\n",
            "Epoch is 143\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2702 - g-acc: 0.9343 - d-loss:: 0.5670 - d-acc:: 0.6870\n",
            "Epoch is 144\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2753 - g-acc: 0.9340 - d-loss:: 0.5895 - d-acc:: 0.6689\n",
            "Epoch is 145\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2840 - g-acc: 0.9205 - d-loss:: 0.5724 - d-acc:: 0.6808\n",
            "Epoch is 146\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2068 - g-acc: 0.9740 - d-loss:: 0.5394 - d-acc:: 0.7119\n",
            "Epoch is 147\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2166 - g-acc: 0.9627 - d-loss:: 0.5600 - d-acc:: 0.6943\n",
            "Epoch is 148\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.3666 - g-acc: 0.8639 - d-loss:: 0.6425 - d-acc:: 0.6187\n",
            "Epoch is 149\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.3270 - g-acc: 0.8966 - d-loss:: 0.6109 - d-acc:: 0.6538\n",
            "Epoch is 150\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.3008 - g-acc: 0.9069 - d-loss:: 0.5816 - d-acc:: 0.6712\n",
            "Epoch is 151\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.2891 - g-acc: 0.9251 - d-loss:: 0.5912 - d-acc:: 0.6654\n",
            "Epoch is 152\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.3596 - g-acc: 0.8614 - d-loss:: 0.6304 - d-acc:: 0.6298\n",
            "Epoch is 153\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.3547 - g-acc: 0.8797 - d-loss:: 0.6109 - d-acc:: 0.6498\n",
            "Epoch is 154\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.3221 - g-acc: 0.9032 - d-loss:: 0.5993 - d-acc:: 0.6561\n",
            "Epoch is 155\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.3421 - g-acc: 0.8794 - d-loss:: 0.5984 - d-acc:: 0.6611\n",
            "Epoch is 156\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.3287 - g-acc: 0.8828 - d-loss:: 0.6182 - d-acc:: 0.6442\n",
            "Epoch is 157\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.3544 - g-acc: 0.8658 - d-loss:: 0.5839 - d-acc:: 0.6782\n",
            "Epoch is 158\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.3715 - g-acc: 0.8595 - d-loss:: 0.6125 - d-acc:: 0.6401\n",
            "Epoch is 159\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.3245 - g-acc: 0.8958 - d-loss:: 0.5966 - d-acc:: 0.6577\n",
            "Epoch is 160\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4324 - g-acc: 0.7990 - d-loss:: 0.6311 - d-acc:: 0.6245\n",
            "Epoch is 161\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.3327 - g-acc: 0.8993 - d-loss:: 0.6049 - d-acc:: 0.6523\n",
            "Epoch is 162\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4433 - g-acc: 0.7824 - d-loss:: 0.6561 - d-acc:: 0.5980\n",
            "Epoch is 163\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4223 - g-acc: 0.8106 - d-loss:: 0.6025 - d-acc:: 0.6489\n",
            "Epoch is 164\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.3304 - g-acc: 0.9098 - d-loss:: 0.5986 - d-acc:: 0.6595\n",
            "Epoch is 165\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.3261 - g-acc: 0.9029 - d-loss:: 0.6083 - d-acc:: 0.6452\n",
            "Epoch is 166\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4563 - g-acc: 0.7837 - d-loss:: 0.6666 - d-acc:: 0.5916\n",
            "Epoch is 167\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.4427 - g-acc: 0.8009 - d-loss:: 0.6223 - d-acc:: 0.6343\n",
            "Epoch is 168\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4017 - g-acc: 0.8438 - d-loss:: 0.6086 - d-acc:: 0.6450\n",
            "Epoch is 169\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4422 - g-acc: 0.8021 - d-loss:: 0.6346 - d-acc:: 0.6190\n",
            "Epoch is 170\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.4248 - g-acc: 0.8113 - d-loss:: 0.6315 - d-acc:: 0.6260\n",
            "Epoch is 171\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4990 - g-acc: 0.7432 - d-loss:: 0.6319 - d-acc:: 0.6171\n",
            "Epoch is 172\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4435 - g-acc: 0.7961 - d-loss:: 0.6351 - d-acc:: 0.6225\n",
            "Epoch is 173\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.3808 - g-acc: 0.8778 - d-loss:: 0.6230 - d-acc:: 0.6363\n",
            "Epoch is 174\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.3251 - g-acc: 0.9194 - d-loss:: 0.6114 - d-acc:: 0.6461\n",
            "Epoch is 175\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4521 - g-acc: 0.7951 - d-loss:: 0.6370 - d-acc:: 0.6198\n",
            "Epoch is 176\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.4159 - g-acc: 0.8421 - d-loss:: 0.6238 - d-acc:: 0.6273\n",
            "Epoch is 177\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4140 - g-acc: 0.8394 - d-loss:: 0.6303 - d-acc:: 0.6222\n",
            "Epoch is 178\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.3657 - g-acc: 0.8747 - d-loss:: 0.6106 - d-acc:: 0.6423\n",
            "Epoch is 179\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4482 - g-acc: 0.8008 - d-loss:: 0.6379 - d-acc:: 0.6142\n",
            "Epoch is 180\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4557 - g-acc: 0.7893 - d-loss:: 0.6241 - d-acc:: 0.6286\n",
            "Epoch is 181\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.3983 - g-acc: 0.8609 - d-loss:: 0.6306 - d-acc:: 0.6218\n",
            "Epoch is 182\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.4231 - g-acc: 0.8284 - d-loss:: 0.6360 - d-acc:: 0.6123\n",
            "Epoch is 183\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.4827 - g-acc: 0.7649 - d-loss:: 0.6218 - d-acc:: 0.6280\n",
            "Epoch is 184\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4687 - g-acc: 0.7955 - d-loss:: 0.6462 - d-acc:: 0.6102\n",
            "Epoch is 185\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5365 - g-acc: 0.7242 - d-loss:: 0.6602 - d-acc:: 0.5955\n",
            "Epoch is 186\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4957 - g-acc: 0.7674 - d-loss:: 0.6357 - d-acc:: 0.6173\n",
            "Epoch is 187\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5137 - g-acc: 0.7372 - d-loss:: 0.6525 - d-acc:: 0.5942\n",
            "Epoch is 188\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.4580 - g-acc: 0.8038 - d-loss:: 0.6271 - d-acc:: 0.6288\n",
            "Epoch is 189\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5350 - g-acc: 0.7268 - d-loss:: 0.6630 - d-acc:: 0.5865\n",
            "Epoch is 190\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5361 - g-acc: 0.7257 - d-loss:: 0.6425 - d-acc:: 0.6090\n",
            "Epoch is 191\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5811 - g-acc: 0.6669 - d-loss:: 0.6623 - d-acc:: 0.5853\n",
            "Epoch is 192\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5453 - g-acc: 0.7081 - d-loss:: 0.6324 - d-acc:: 0.6206\n",
            "Epoch is 193\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5161 - g-acc: 0.7474 - d-loss:: 0.6533 - d-acc:: 0.5997\n",
            "Epoch is 194\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5275 - g-acc: 0.7339 - d-loss:: 0.6552 - d-acc:: 0.5959\n",
            "Epoch is 195\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5521 - g-acc: 0.7096 - d-loss:: 0.6526 - d-acc:: 0.5937\n",
            "Epoch is 196\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5585 - g-acc: 0.6947 - d-loss:: 0.6564 - d-acc:: 0.5870\n",
            "Epoch is 197\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5788 - g-acc: 0.6735 - d-loss:: 0.6497 - d-acc:: 0.6019\n",
            "Epoch is 198\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5652 - g-acc: 0.6875 - d-loss:: 0.6670 - d-acc:: 0.5802\n",
            "Epoch is 199\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5761 - g-acc: 0.6905 - d-loss:: 0.6625 - d-acc:: 0.5907\n",
            "Epoch is 200\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4907 - g-acc: 0.7787 - d-loss:: 0.6162 - d-acc:: 0.6362\n",
            "Epoch is 201\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.4759 - g-acc: 0.7989 - d-loss:: 0.6375 - d-acc:: 0.6166\n",
            "Epoch is 202\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5845 - g-acc: 0.6660 - d-loss:: 0.6416 - d-acc:: 0.6081\n",
            "Epoch is 203\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5382 - g-acc: 0.7204 - d-loss:: 0.6294 - d-acc:: 0.6242\n",
            "Epoch is 204\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5189 - g-acc: 0.7312 - d-loss:: 0.6442 - d-acc:: 0.6014\n",
            "Epoch is 205\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5363 - g-acc: 0.7207 - d-loss:: 0.6453 - d-acc:: 0.6076\n",
            "Epoch is 206\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5389 - g-acc: 0.7241 - d-loss:: 0.6510 - d-acc:: 0.5962\n",
            "Epoch is 207\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.4885 - g-acc: 0.7760 - d-loss:: 0.6435 - d-acc:: 0.6016\n",
            "Epoch is 208\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5489 - g-acc: 0.7020 - d-loss:: 0.6596 - d-acc:: 0.5824\n",
            "Epoch is 209\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5794 - g-acc: 0.6667 - d-loss:: 0.6557 - d-acc:: 0.5917\n",
            "Epoch is 210\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5937 - g-acc: 0.6637 - d-loss:: 0.6494 - d-acc:: 0.6039\n",
            "Epoch is 211\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5891 - g-acc: 0.6688 - d-loss:: 0.6649 - d-acc:: 0.5797\n",
            "Epoch is 212\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5752 - g-acc: 0.6821 - d-loss:: 0.6501 - d-acc:: 0.5967\n",
            "Epoch is 213\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5494 - g-acc: 0.7199 - d-loss:: 0.6261 - d-acc:: 0.6269\n",
            "Epoch is 214\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5225 - g-acc: 0.7387 - d-loss:: 0.6348 - d-acc:: 0.6192\n",
            "Epoch is 215\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.6064 - g-acc: 0.6446 - d-loss:: 0.6682 - d-acc:: 0.5746\n",
            "Epoch is 216\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5676 - g-acc: 0.6935 - d-loss:: 0.6325 - d-acc:: 0.6186\n",
            "Epoch is 217\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5537 - g-acc: 0.7166 - d-loss:: 0.6409 - d-acc:: 0.6081\n",
            "Epoch is 218\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5486 - g-acc: 0.7095 - d-loss:: 0.6483 - d-acc:: 0.5989\n",
            "Epoch is 219\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5731 - g-acc: 0.6873 - d-loss:: 0.6526 - d-acc:: 0.5936\n",
            "Epoch is 220\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5943 - g-acc: 0.6509 - d-loss:: 0.6500 - d-acc:: 0.5938\n",
            "Epoch is 221\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5576 - g-acc: 0.7027 - d-loss:: 0.6465 - d-acc:: 0.6052\n",
            "Epoch is 222\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5981 - g-acc: 0.6380 - d-loss:: 0.6463 - d-acc:: 0.6008\n",
            "Epoch is 223\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5194 - g-acc: 0.7412 - d-loss:: 0.6397 - d-acc:: 0.6093\n",
            "Epoch is 224\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.6180 - g-acc: 0.6333 - d-loss:: 0.6519 - d-acc:: 0.5991\n",
            "Epoch is 225\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5380 - g-acc: 0.7255 - d-loss:: 0.6343 - d-acc:: 0.6133\n",
            "Epoch is 226\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5173 - g-acc: 0.7554 - d-loss:: 0.6450 - d-acc:: 0.6074\n",
            "Epoch is 227\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5053 - g-acc: 0.7650 - d-loss:: 0.6327 - d-acc:: 0.6167\n",
            "Epoch is 228\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5383 - g-acc: 0.7167 - d-loss:: 0.6290 - d-acc:: 0.6192\n",
            "Epoch is 229\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4592 - g-acc: 0.7989 - d-loss:: 0.6298 - d-acc:: 0.6243\n",
            "Epoch is 230\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5575 - g-acc: 0.6952 - d-loss:: 0.6438 - d-acc:: 0.6058\n",
            "Epoch is 231\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5716 - g-acc: 0.7010 - d-loss:: 0.6444 - d-acc:: 0.6079\n",
            "Epoch is 232\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4670 - g-acc: 0.8219 - d-loss:: 0.6230 - d-acc:: 0.6375\n",
            "Epoch is 233\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5014 - g-acc: 0.7590 - d-loss:: 0.6357 - d-acc:: 0.6129\n",
            "Epoch is 234\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5708 - g-acc: 0.6832 - d-loss:: 0.6421 - d-acc:: 0.6062\n",
            "Epoch is 235\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.4867 - g-acc: 0.7904 - d-loss:: 0.6322 - d-acc:: 0.6211\n",
            "Epoch is 236\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5895 - g-acc: 0.6539 - d-loss:: 0.6312 - d-acc:: 0.6161\n",
            "Epoch is 237\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.6044 - g-acc: 0.6476 - d-loss:: 0.6355 - d-acc:: 0.6127\n",
            "Epoch is 238\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5032 - g-acc: 0.7575 - d-loss:: 0.6343 - d-acc:: 0.6164\n",
            "Epoch is 239\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5462 - g-acc: 0.7285 - d-loss:: 0.6470 - d-acc:: 0.6083\n",
            "Epoch is 240\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5174 - g-acc: 0.7704 - d-loss:: 0.6246 - d-acc:: 0.6353\n",
            "Epoch is 241\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5438 - g-acc: 0.7079 - d-loss:: 0.6393 - d-acc:: 0.6115\n",
            "Epoch is 242\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5382 - g-acc: 0.7236 - d-loss:: 0.6449 - d-acc:: 0.6077\n",
            "Epoch is 243\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5706 - g-acc: 0.6768 - d-loss:: 0.6403 - d-acc:: 0.6050\n",
            "Epoch is 244\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5628 - g-acc: 0.6924 - d-loss:: 0.6279 - d-acc:: 0.6234\n",
            "Epoch is 245\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5686 - g-acc: 0.6933 - d-loss:: 0.6400 - d-acc:: 0.6104\n",
            "Epoch is 246\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.4799 - g-acc: 0.8056 - d-loss:: 0.6382 - d-acc:: 0.6211\n",
            "Epoch is 247\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.3265 - g-acc: 0.9131 - d-loss:: 0.5779 - d-acc:: 0.6761\n",
            "Epoch is 248\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.3482 - g-acc: 0.8929 - d-loss:: 0.5961 - d-acc:: 0.6592\n",
            "Epoch is 249\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5619 - g-acc: 0.6874 - d-loss:: 0.6716 - d-acc:: 0.5779\n",
            "Epoch is 250\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5804 - g-acc: 0.6739 - d-loss:: 0.6623 - d-acc:: 0.5821\n",
            "Epoch is 251\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5446 - g-acc: 0.7138 - d-loss:: 0.6337 - d-acc:: 0.6154\n",
            "Epoch is 252\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5393 - g-acc: 0.7248 - d-loss:: 0.6261 - d-acc:: 0.6280\n",
            "Epoch is 253\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5160 - g-acc: 0.7322 - d-loss:: 0.6254 - d-acc:: 0.6270\n",
            "Epoch is 254\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4960 - g-acc: 0.7521 - d-loss:: 0.6353 - d-acc:: 0.6094\n",
            "Epoch is 255\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5271 - g-acc: 0.7297 - d-loss:: 0.6480 - d-acc:: 0.6011\n",
            "Epoch is 256\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5026 - g-acc: 0.7609 - d-loss:: 0.6478 - d-acc:: 0.6031\n",
            "Epoch is 257\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5459 - g-acc: 0.7228 - d-loss:: 0.6385 - d-acc:: 0.6216\n",
            "Epoch is 258\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5887 - g-acc: 0.6675 - d-loss:: 0.6615 - d-acc:: 0.5920\n",
            "Epoch is 259\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5908 - g-acc: 0.6541 - d-loss:: 0.6504 - d-acc:: 0.5972\n",
            "Epoch is 260\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5078 - g-acc: 0.7588 - d-loss:: 0.6355 - d-acc:: 0.6182\n",
            "Epoch is 261\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5529 - g-acc: 0.6954 - d-loss:: 0.6218 - d-acc:: 0.6217\n",
            "Epoch is 262\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4398 - g-acc: 0.8428 - d-loss:: 0.6146 - d-acc:: 0.6423\n",
            "Epoch is 263\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5869 - g-acc: 0.6644 - d-loss:: 0.6455 - d-acc:: 0.6055\n",
            "Epoch is 264\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5320 - g-acc: 0.7348 - d-loss:: 0.6338 - d-acc:: 0.6160\n",
            "Epoch is 265\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4447 - g-acc: 0.8500 - d-loss:: 0.5802 - d-acc:: 0.6759\n",
            "Epoch is 266\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5004 - g-acc: 0.7608 - d-loss:: 0.6143 - d-acc:: 0.6410\n",
            "Epoch is 267\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.4189 - g-acc: 0.8389 - d-loss:: 0.6218 - d-acc:: 0.6305\n",
            "Epoch is 268\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5726 - g-acc: 0.6816 - d-loss:: 0.6638 - d-acc:: 0.5860\n",
            "Epoch is 269\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5079 - g-acc: 0.7627 - d-loss:: 0.6336 - d-acc:: 0.6181\n",
            "Epoch is 270\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.4563 - g-acc: 0.7992 - d-loss:: 0.6192 - d-acc:: 0.6311\n",
            "Epoch is 271\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5520 - g-acc: 0.6946 - d-loss:: 0.6342 - d-acc:: 0.6154\n",
            "Epoch is 272\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.3785 - g-acc: 0.8950 - d-loss:: 0.5797 - d-acc:: 0.6805\n",
            "Epoch is 273\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5305 - g-acc: 0.7204 - d-loss:: 0.6641 - d-acc:: 0.5952\n",
            "Epoch is 274\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5519 - g-acc: 0.7033 - d-loss:: 0.6437 - d-acc:: 0.6119\n",
            "Epoch is 275\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5177 - g-acc: 0.7572 - d-loss:: 0.6410 - d-acc:: 0.6152\n",
            "Epoch is 276\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5619 - g-acc: 0.7047 - d-loss:: 0.6405 - d-acc:: 0.6156\n",
            "Epoch is 277\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5711 - g-acc: 0.6831 - d-loss:: 0.6477 - d-acc:: 0.6036\n",
            "Epoch is 278\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4925 - g-acc: 0.7779 - d-loss:: 0.6023 - d-acc:: 0.6610\n",
            "Epoch is 279\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.4990 - g-acc: 0.7617 - d-loss:: 0.6228 - d-acc:: 0.6300\n",
            "Epoch is 280\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4839 - g-acc: 0.7799 - d-loss:: 0.6227 - d-acc:: 0.6343\n",
            "Epoch is 281\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4443 - g-acc: 0.7991 - d-loss:: 0.5994 - d-acc:: 0.6582\n",
            "Epoch is 282\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.3385 - g-acc: 0.8959 - d-loss:: 0.5939 - d-acc:: 0.6655\n",
            "Epoch is 283\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.6475 - g-acc: 0.6074 - d-loss:: 0.6499 - d-acc:: 0.6038\n",
            "Epoch is 284\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5852 - g-acc: 0.6668 - d-loss:: 0.6528 - d-acc:: 0.6027\n",
            "Epoch is 285\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.4751 - g-acc: 0.8068 - d-loss:: 0.6053 - d-acc:: 0.6579\n",
            "Epoch is 286\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4865 - g-acc: 0.7763 - d-loss:: 0.6097 - d-acc:: 0.6464\n",
            "Epoch is 287\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4652 - g-acc: 0.7960 - d-loss:: 0.6213 - d-acc:: 0.6297\n",
            "Epoch is 288\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5134 - g-acc: 0.7400 - d-loss:: 0.6430 - d-acc:: 0.6098\n",
            "Epoch is 289\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5501 - g-acc: 0.7003 - d-loss:: 0.6211 - d-acc:: 0.6322\n",
            "Epoch is 290\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4904 - g-acc: 0.7725 - d-loss:: 0.6238 - d-acc:: 0.6299\n",
            "Epoch is 291\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.3934 - g-acc: 0.8646 - d-loss:: 0.5981 - d-acc:: 0.6595\n",
            "Epoch is 292\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4411 - g-acc: 0.8133 - d-loss:: 0.6003 - d-acc:: 0.6567\n",
            "Epoch is 293\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4116 - g-acc: 0.8288 - d-loss:: 0.5968 - d-acc:: 0.6571\n",
            "Epoch is 294\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.5665 - g-acc: 0.6724 - d-loss:: 0.6443 - d-acc:: 0.6067\n",
            "Epoch is 295\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5901 - g-acc: 0.6878 - d-loss:: 0.6317 - d-acc:: 0.6300\n",
            "Epoch is 296\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5400 - g-acc: 0.7159 - d-loss:: 0.6280 - d-acc:: 0.6215\n",
            "Epoch is 297\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4786 - g-acc: 0.7713 - d-loss:: 0.6166 - d-acc:: 0.6298\n",
            "Epoch is 298\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.5163 - g-acc: 0.7489 - d-loss:: 0.6282 - d-acc:: 0.6253\n",
            "Epoch is 299\n",
            "83/83 [==============================] - 97s 1s/step - g-loss:: 0.4584 - g-acc: 0.8005 - d-loss:: 0.6134 - d-acc:: 0.6441\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzmQ6vnF8bmw",
        "colab_type": "text"
      },
      "source": [
        "## Anamoly Detection, Approach - 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhIzmfdfH7_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def anomaly_detection(test_img, g=None, d=None):\n",
        "    model = anomaly_detector(g=g, d=d)\n",
        "    ano_score, similar_img = compute_anomaly_score(model, test_img.reshape(1, 256, 256, 1), iterations=500, d=d)\n",
        "\n",
        "    # anomaly area, 255 normalization\n",
        "    np_residual = test_img.reshape(256,256,1) - similar_img.reshape(256,256,1)\n",
        "    np_residual = (np_residual + 2)/4\n",
        "\n",
        "    np_residual = (255*np_residual).astype(np.uint8)\n",
        "    original_x = (test_img.reshape(256,256,1)*127.5+127.5).astype(np.uint8)\n",
        "    similar_x = (similar_img.reshape(256,256,1)*127.5+127.5).astype(np.uint8)\n",
        "\n",
        "    original_x_color = cv2.cvtColor(original_x, cv2.COLOR_GRAY2BGR)\n",
        "    residual_color = cv2.applyColorMap(np_residual, cv2.COLORMAP_JET)\n",
        "    show = cv2.addWeighted(original_x_color, 0.3, residual_color, 0.7, 0.)\n",
        "\n",
        "    return ano_score, original_x, similar_x, show\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebKDYKJE0_tj",
        "colab_type": "code",
        "outputId": "6587fec5-546c-4dd6-8d0d-ca1e6d50dba0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#test_image_batch = (natural_validation_generator.next()[0:5].astype(np.float32) - 127.5) / 127.5\n",
        "#for image in test_image_batch:\n",
        "#    img_idx = 0\n",
        "#    start = cv2.getTickCount()\n",
        "#    score, qurey, pred, diff = anomaly_detection(image, Model_g, Model_d)\n",
        "#    time = (cv2.getTickCount() - start) / cv2.getTickFrequency() * 1000\n",
        "#    print ('techno image: %d : done'%( img_idx), 'Score: %.2f'%score, 'Time:%.2fms'%time)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "techno image: 0 : done Score: 12296.56 Time:9583.61ms\n",
            "techno image: 0 : done Score: 21023.97 Time:9535.10ms\n",
            "techno image: 0 : done Score: 18588.28 Time:9842.25ms\n",
            "techno image: 0 : done Score: 15228.32 Time:10356.36ms\n",
            "techno image: 0 : done Score: 19894.46 Time:10160.00ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJbOo8WvRyiu",
        "colab_type": "code",
        "outputId": "de832ed1-0843-47bb-d3ca-3ab4c2c6030b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#test_image_batch = (techno_validation_generator.next()[0:5].astype(np.float32) - 127.5) / 127.5\n",
        "#for image in test_image_batch:\n",
        "#    img_idx = 0\n",
        "#    start = cv2.getTickCount()\n",
        "#    score, qurey, pred, diff = anomaly_detection(image, Model_g, Model_d)\n",
        "#    time = (cv2.getTickCount() - start) / cv2.getTickFrequency() * 1000\n",
        "#    print ('techno image: %d : done'%( img_idx), 'Score: %.2f'%score, 'Time:%.2fms'%time)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "techno image: 0 : done Score: 23380.92 Time:8456.15ms\n",
            "techno image: 0 : done Score: 12817.43 Time:8430.62ms\n",
            "techno image: 0 : done Score: 53128.41 Time:8832.68ms\n",
            "techno image: 0 : done Score: 8442.28 Time:9100.25ms\n",
            "techno image: 0 : done Score: 22049.61 Time:9794.49ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-xYDNGBo8MR",
        "colab_type": "code",
        "outputId": "55166806-bdba-4ab5-87f7-a7c7189b4c16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#test_image_batch.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 256, 256, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG_6BKA711mj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "#\n",
        "#plt.figure(1, figsize=(3, 3))\n",
        "#plt.title('query image')\n",
        "#plt.imshow(qurey.reshape(64,64), cmap=plt.cm.gray)\n",
        "#\n",
        "#print(\"anomaly score : \", score)\n",
        "#plt.figure(2, figsize=(3, 3))\n",
        "#plt.title('generated similar image')\n",
        "#plt.imshow(pred.reshape(64,64), cmap=plt.cm.gray)\n",
        "#\n",
        "#plt.figure(3, figsize=(3, 3))\n",
        "#plt.title('anomaly detection')\n",
        "#plt.imshow(cv2.cvtColor(diff,cv2.COLOR_BGR2RGB))\n",
        "#plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRVY5YoM8h4h",
        "colab_type": "text"
      },
      "source": [
        "## Anamoly Detection, Approach - 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkpcSQbO6YCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def detect_defects(discriminator_model, validation_generator, threshold=0.5, verbose=1):\n",
        "\n",
        "        total_samples = validation_generator.samples\n",
        "        batch_size = validation_generator.batch_size\n",
        "\n",
        "        results = list()\n",
        "        labels = list()\n",
        "\n",
        "        if (verbose != 0):\n",
        "            progress_bar = Progbar(target=total_samples)\n",
        "\n",
        "        for _ in range(np.ceil(total_samples / batch_size).astype(np.int32)):\n",
        "\n",
        "            image_batch, lbls = validation_generator.next()\n",
        "\n",
        "            labels = np.append(labels, lbls.reshape(lbls.shape[0]))\n",
        "            image_batch = (image_batch.astype(np.float32) - 127.5) / 127.5\n",
        "\n",
        "            tmp_rslt = discriminator_model.predict(\n",
        "                x=image_batch,\n",
        "                batch_size=image_batch.shape[0],\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            if (verbose != 0):\n",
        "                progress_bar.add(image_batch.shape[0])\n",
        "\n",
        "            results = np.append(results, tmp_rslt.reshape(tmp_rslt.shape[0]))\n",
        "\n",
        "        results = [1 if x >= threshold else 0 for x in results]\n",
        "\n",
        "        tn, fp, fn, tp = confusion_matrix(labels, results).ravel()\n",
        "        print(\"tn:\", tn, \"fp:\", fp, \"fn:\", fn, \"tp:\", tp)\n",
        "\n",
        "        #################### NON DEFECT SITUATIONS ####################\n",
        "\n",
        "        # Probability of Detecting a Non-Defect: (tp / (tp + fn))\n",
        "        if ((tp + fn) != 0):\n",
        "            recall = tp / (tp + fn)\n",
        "        else:\n",
        "            recall = 0.0\n",
        "\n",
        "        # Probability of Correctly Detecting a Non-Defect: (tp / (tp + fp))\n",
        "\n",
        "        if ((tp + fp) != 0):\n",
        "            precision = tp / (tp + fp)\n",
        "        else:\n",
        "            precision = 0.0\n",
        "\n",
        "        ###################### DEFECT SITUATIONS ######################\n",
        "\n",
        "        # Probability of Detecting a Defect: (tn / (tn + fp))\n",
        "        if ((tn + fp) != 0):\n",
        "            specificity = tn / (tn + fp)\n",
        "        else:\n",
        "            specificity = 0.0\n",
        "\n",
        "        # Probability of Correctly Detecting a Defect: (tn / (tn + fn))\n",
        "        if ((tn + fn) != 0):\n",
        "            negative_predictive_value = tn / (tn + fn)\n",
        "        else:\n",
        "            negative_predictive_value = 0.0\n",
        "\n",
        "        return precision, recall, specificity, negative_predictive_value\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyqL-Qn768b6",
        "colab_type": "code",
        "outputId": "e0b84f76-5e7f-4059-e778-1539127f8b64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "# this is a similar generator, for validation data\n",
        "test_datagen = ImageDataGenerator()\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    directory   = validation_directory,     # this is the target directory\n",
        "    target_size = (128, 128, 1)[:-1],         # all images will be resized to 64x64\n",
        "    batch_size  = 64,\n",
        "    color_mode  = \"grayscale\",              # We use a grayscale dataset\n",
        "    classes     = [\"natural\", \"techno\"],        # {'docs': 0, 'other': 1} => Needs to be enforced\n",
        "    class_mode  = 'binary'                  # We want to have binary labels for validation\n",
        ")\n",
        "\n",
        "\n",
        "#predict\n",
        "d = load_discriminator_model(best=False)\n",
        "precision, recall, specificity, negative_predictive_value = detect_defects(d, validation_generator,threshold=0.3)\n",
        "\n",
        "print(\"Probability of Detecting a Non-Defect             => Recall      (tp / (tp + fn)):\", recall)\n",
        "print(\"Probability of Correctly Detecting a Non-Defect   => Precision   (tp / (tp + fp)):\", precision)\n",
        "print(\"Probability of Detecting a Defect                 => Specificity (tn / (tn + fp)):\", specificity)\n",
        "print(\"Probability of Correctly Detecting a Defect       => NPV         (tn / (tn + fn)):\", negative_predictive_value)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 114 images belonging to 2 classes.\n",
            "114/114 [==============================] - 0s 3ms/step\n",
            "tn: 56 fp: 8 fn: 40 tp: 10\n",
            "Probability of Detecting a Non-Defect             => Recall      (tp / (tp + fn)): 0.2\n",
            "Probability of Correctly Detecting a Non-Defect   => Precision   (tp / (tp + fp)): 0.5555555555555556\n",
            "Probability of Detecting a Defect                 => Specificity (tn / (tn + fp)): 0.875\n",
            "Probability of Correctly Detecting a Defect       => NPV         (tn / (tn + fn)): 0.5833333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGVdexRGL8OJ",
        "colab_type": "code",
        "outputId": "f59ef9c6-dce0-4028-ea0d-ac4ab283204e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "validation_generator.class_indices"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'natural': 1, 'techno': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_lmz7AXNoOm",
        "colab_type": "code",
        "outputId": "7f920764-c960-47d4-fc3e-c502d7dcd2eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 926
        }
      },
      "source": [
        "from numpy import newaxis\n",
        "folder=validation_directory+\"techno\"\n",
        "\n",
        "technoPredictions = []\n",
        "for filename in os.listdir(folder):\n",
        "    img = cv2.imread(os.path.join(folder,filename),0)\n",
        "    img = cv2.resize(img,(128,128))\n",
        "    img = (img.astype(np.float32) - 127.5) / 127.5\n",
        "    im2 = np.array([img])\n",
        "    im2 = im2[:,:,:,newaxis]\n",
        "    #print(im2.shape)\n",
        "    tmp_rslt = d.predict(x=im2,\n",
        "                batch_size=1,\n",
        "                verbose=0\n",
        "            )\n",
        "    technoPredictions.append(tmp_rslt[0][0])\n",
        "    print(filename, tmp_rslt[0][0])\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "img_x_7552_y_22400.jpg 1.0\n",
            "img_x_6912_y_20608.jpg 0.9883857\n",
            "img_x_7296_y_22144.jpg 0.31642044\n",
            "img_x_15488_y_22528.jpg 0.9999914\n",
            "img_x_15616_y_22656.jpg 0.99998665\n",
            "img_x_7552_y_22144.jpg 1.0\n",
            "img_x_6528_y_19584.jpg 0.15875597\n",
            "img_x_6400_y_19584.jpg 0.20403388\n",
            "img_x_6656_y_19584.jpg 0.21504915\n",
            "img_x_7424_y_22272.jpg 1.0\n",
            "img_x_6656_y_19456.jpg 0.06714094\n",
            "img_x_7040_y_20480.jpg 0.9870509\n",
            "img_x_9728_y_21376.jpg 0.9757642\n",
            "img_x_6784_y_20736.jpg 0.1392769\n",
            "img_x_6528_y_19712.jpg 0.24852754\n",
            "img_x_6784_y_20480.jpg 0.13802381\n",
            "img_x_7552_y_22272.jpg 1.0\n",
            "img_x_9728_y_21504.jpg 0.92922115\n",
            "img_x_7040_y_20608.jpg 0.9585537\n",
            "img_x_7168_y_20480.jpg 0.6556935\n",
            "img_x_7296_y_22400.jpg 0.071307056\n",
            "img_x_15616_y_22528.jpg 0.9999937\n",
            "img_x_7296_y_20608.jpg 0.79291654\n",
            "img_x_15360_y_22656.jpg 0.063316256\n",
            "img_x_6784_y_20608.jpg 0.18692394\n",
            "img_x_7424_y_22144.jpg 1.0\n",
            "img_x_7680_y_22272.jpg 0.5158083\n",
            "img_x_9600_y_21504.jpg 0.8906618\n",
            "img_x_6656_y_19712.jpg 0.17712303\n",
            "img_x_15488_y_22656.jpg 0.99998844\n",
            "img_x_7168_y_20352.jpg 0.69118655\n",
            "img_x_6400_y_19712.jpg 0.42864797\n",
            "img_x_7168_y_20736.jpg 0.60555464\n",
            "img_x_7424_y_22400.jpg 1.0\n",
            "img_x_7168_y_20608.jpg 0.6852349\n",
            "img_x_9600_y_21376.jpg 0.94565016\n",
            "img_x_6912_y_20736.jpg 0.6344467\n",
            "img_x_6528_y_19456.jpg 0.065867215\n",
            "img_x_7296_y_20352.jpg 0.2427961\n",
            "img_x_6912_y_20352.jpg 0.5181704\n",
            "img_x_6784_y_20352.jpg 0.14773753\n",
            "img_x_7296_y_22272.jpg 0.029392548\n",
            "img_x_7680_y_22400.jpg 0.09613475\n",
            "img_x_7680_y_22144.jpg 0.35092196\n",
            "img_x_7040_y_20736.jpg 0.9690745\n",
            "img_x_7040_y_20352.jpg 0.64472204\n",
            "img_x_6912_y_20480.jpg 0.88757324\n",
            "img_x_7296_y_20736.jpg 0.66728276\n",
            "img_x_15360_y_22528.jpg 0.1101865\n",
            "img_x_7296_y_20480.jpg 0.8406859\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0-PAWnmnaBU",
        "colab_type": "code",
        "outputId": "1c4e0446-d770-42e0-fe97-34ed02a7d786",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1181
        }
      },
      "source": [
        "from numpy import newaxis\n",
        "folder=validation_directory+\"natural\"\n",
        "\n",
        "naturalPredictions = []\n",
        "for filename in os.listdir(folder):\n",
        "    img = cv2.imread(os.path.join(folder,filename),0)\n",
        "    img = cv2.resize(img,(128,128))\n",
        "    img = (img.astype(np.float32) - 127.5) / 127.5\n",
        "    im2 = np.array([img])\n",
        "    im2 = im2[:,:,:,newaxis]\n",
        "    #print(im2.shape)\n",
        "    tmp_rslt = d.predict(x=im2,\n",
        "                batch_size=1,\n",
        "                verbose=0\n",
        "            )\n",
        "    naturalPredictions.append(tmp_rslt[0][0])\n",
        "    print(filename, tmp_rslt[0][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "img_x_10752_y_17408.jpg 0.14697611\n",
            "img_x_11648_y_16896.jpg 0.19080912\n",
            "img_x_13568_y_7808.jpg 0.14173661\n",
            "img_x_14336_y_18688.jpg 0.22423638\n",
            "img_x_9472_y_29184.jpg 0.9882239\n",
            "img_x_10368_y_8960.jpg 0.20709567\n",
            "img_x_11648_y_21504.jpg 0.019217575\n",
            "img_x_14720_y_8448.jpg 0.3724327\n",
            "img_x_11520_y_9984.jpg 0.36900547\n",
            "img_x_14720_y_8192.jpg 0.074692704\n",
            "img_x_12288_y_13440.jpg 0.4571508\n",
            "img_x_10752_y_7552.jpg 0.26875517\n",
            "img_x_13824_y_17408.jpg 0.06876851\n",
            "img_x_9728_y_10880.jpg 0.1346322\n",
            "img_x_9856_y_18176.jpg 0.014412622\n",
            "img_x_9856_y_22016.jpg 0.13594581\n",
            "img_x_11648_y_9984.jpg 0.6636361\n",
            "img_x_10240_y_25344.jpg 0.27711833\n",
            "img_x_12672_y_18176.jpg 0.098404996\n",
            "img_x_14976_y_13184.jpg 0.059294216\n",
            "img_x_11264_y_29440.jpg 0.9999976\n",
            "img_x_9216_y_28544.jpg 0.3732963\n",
            "img_x_9472_y_9728.jpg 0.5726497\n",
            "img_x_9472_y_6784.jpg 0.14794983\n",
            "img_x_9600_y_9088.jpg 0.21641523\n",
            "img_x_12288_y_29824.jpg 0.9999727\n",
            "img_x_12032_y_18688.jpg 0.091706984\n",
            "img_x_8960_y_6144.jpg 0.08773631\n",
            "img_x_11776_y_13056.jpg 0.045334373\n",
            "img_x_9216_y_7424.jpg 0.30454227\n",
            "img_x_8064_y_10240.jpg 0.20983566\n",
            "img_x_8832_y_18688.jpg 0.011863927\n",
            "img_x_12160_y_13440.jpg 0.36512658\n",
            "img_x_8960_y_28032.jpg 0.14859019\n",
            "img_x_12800_y_17408.jpg 0.023274183\n",
            "img_x_11392_y_17792.jpg 0.15377447\n",
            "img_x_9728_y_10368.jpg 0.56292665\n",
            "img_x_8320_y_9984.jpg 0.07900329\n",
            "img_x_12032_y_7808.jpg 0.47170514\n",
            "img_x_11136_y_21888.jpg 0.3286\n",
            "img_x_11904_y_8192.jpg 0.24225633\n",
            "img_x_9984_y_25728.jpg 0.1899763\n",
            "img_x_11264_y_5248.jpg 0.25396875\n",
            "img_x_14720_y_5888.jpg 0.13787867\n",
            "img_x_8064_y_13184.jpg 0.07925973\n",
            "img_x_9600_y_20224.jpg 0.044926874\n",
            "img_x_9600_y_17024.jpg 0.13444531\n",
            "img_x_11648_y_10368.jpg 0.050922308\n",
            "img_x_10752_y_6784.jpg 0.44476157\n",
            "img_x_9216_y_25984.jpg 0.14460796\n",
            "img_x_14592_y_19456.jpg 0.30598563\n",
            "img_x_11392_y_13696.jpg 0.23397698\n",
            "img_x_9856_y_20736.jpg 0.5714079\n",
            "img_x_11520_y_7680.jpg 0.15653525\n",
            "img_x_12032_y_22144.jpg 0.09517314\n",
            "img_x_12544_y_5248.jpg 0.16616157\n",
            "img_x_12160_y_22272.jpg 0.2805331\n",
            "img_x_8576_y_8960.jpg 0.2444867\n",
            "img_x_11648_y_27520.jpg 0.6211655\n",
            "img_x_8192_y_29824.jpg 0.32684308\n",
            "img_x_9984_y_7424.jpg 0.14255606\n",
            "img_x_13440_y_18688.jpg 0.0108824\n",
            "img_x_10240_y_26240.jpg 0.088054284\n",
            "img_x_12416_y_7808.jpg 0.15235864\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjklyXcJ8JA4",
        "colab_type": "code",
        "outputId": "932ea21c-f65d-4444-d78d-d769c2bd9364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({'natural':naturalPredictions[:50],'techno':technoPredictions})\n",
        "df.boxplot(column=['natural','techno'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7efe7140f4e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGmS4S1KD/g8F1l8H5NPEvQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4qg-84mQgCh",
        "colab_type": "code",
        "outputId": "7263500d-39fe-476f-d46c-b5df4051273e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('img_x_10752_y_17408.jpg', 0.012893274)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVLE89me9bMn",
        "colab_type": "code",
        "outputId": "1926a827-0c5a-4300-fd43-b55b5f1b489e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "source": [
        "df.hist()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7efe713449e8>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7efe713029e8>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NiBbTrnBRMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}